<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
"http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd"
[
	<!ENTITY % wdb.entities SYSTEM "../docbook/wdb_entities.ent">
	%wdb.entities;
]
>

<chapter id="cha:wdb_test-plan">

  <title>Test Plan</title>

	<sect1><title>Introduction</title>

<para>This document describes the overall test plan to be followed for the development and maintenance of the WDB system.</para>

<sect2><title>Purpose</title>

<para>The purpose of this document is to describe the test plan for the WDB system. Specifically:</para>

<itemizedlist>
      <listitem>
    <para>Document the consensus for the level of testing maintained for the WDB system</para></listitem>
</itemizedlist>

<para>The tests set forth in this document are designed to satisfy the demands of three of the stakeholder groups in the WDB system:</para>

<itemizedlist>
      <listitem>
     <para>the WDB development group</para>
     <para>A strong test framework makes it easier to develop the system, as changes can easily be verified using regression testing.</para></listitem>
    <listitem><para>the WDB operations group</para>
      <para>A strong test framework dramatically reduces the number of bugs that make it into production, thus reducing the workload of the operations group.</para></listitem>
    <listitem><para>users of Diana</para>
      <para>The test-framework provides the guarantee for the end-users that the product they are utilizing achieves a given level of quality.</para></listitem>
</itemizedlist>

</sect2>

</sect1>

<sect1><title>Test Methods</title>

<para>In this section, we briefly describe the various test methods
used to ensure the quality of the WDB system, the purpose of each, and
how they are documented, implemented, and performed.</para>

<para>The test methods that are utilized in the WDB system are:</para>

<itemizedlist>
    <listitem><para>Compilation testing</para></listitem>
    <listitem><para>Unit testing</para></listitem>
    <listitem><para>Code reviews</para></listitem>
    <listitem><para>Installation testing</para></listitem>
    <listitem><para>Distribution testing</para></listitem>
    <listitem><para>Performance testing</para></listitem>
    <listitem><para>Stability testing</para></listitem>
</itemizedlist>

<sect2><title>Compilation Testing</title>

<para>Compilation testing is simply a basic test to insure that the
system compiles and is linked correctly. No warnings should be
generated by the compilation of any component of the WDB
system.</para>

<para>Performing compilation testing is the responsibility of the
systems developer.</para>

</sect2>

<sect2><title>Unit Testing</title>

<para>Unit testing is a method used to validate that individual units
of source code are work properly. A unit is the smallest testable part
of a component; e.g., an individual procedure, function, or class. The
purpose of unit testing is to isolate parts of the program and
demonstrate that they are correct, and doing so simplifies integration
and facilitate refactoring and change.</para>

<para>Unit testing is first and foremost a method leveraged on the
core (calculation) functionality, as the method is less useful for
input/output functionality. For this reason, care must be taken to
separate the interface (e.g., database connection, file reads) from
the implementation.</para>

<para>Unit tests are constructed, as needed, by the systems developer; preferably before the code being tested. For C++ code, the unit tests should be implemented using CppUnit [1] and documented using Javadoc tags in the test code.</para>

<para>Unit tests are run using ``make check''.</para>

<para>Performing unit testing is the responsibility of the systems
developer.</para>

</sect2>

<sect2><title>Code Reviews</title>

<para>All code to be placed into production must undergo systematic examination (peer review). The purpose of the code review is to find and fix mistakes, thus improving the overall quality of the software, as well as to help the developer improve his skills.</para>

<para>Ensuring that new code is reviewed is the responsibility of the systems developer.</para>

</sect2>

<sect2><title>Installation Testing</title>

<para>Installation testing in the WDB system handles the testing of WDB components against an installed version of the database. The purpose of these tests is to examine that the functionality of the components fulfills the system requirements of WDB.</para>

<para>The functionality of each component should be tested using the Category-Partition test design pattern [2]. Ideally, each functional requirement of a WDB component should have an associated install test.</para>

<para>Install tests are run using ``make installcheck''.</para>

<para>Installation testing is the responsibility of the systems tester
and is usually carried out in connection with the distribution test
(cf. <xref linkend="sec:distribution-testing"/>).</para>

</sect2>

<sect2 id="sec:distribution-testing"><title>Distribution Testing</title>

<para>Distribution testing verifies that the WDB system is ready for distribution. It verifies that a distribution package for the system can be created, compiled and installed, and runs both unit and installation tests on the resulting installation.</para>

<para>Distribution tests are run using ``make distcheck''.</para>

<para>Distribution testing is the responsibility of the systems tester.</para>

</sect2>

<sect2><title>Performance Testing</title>

<para>Performance testing verifies that WDB live up to the performance specifications of the system.</para>

<para>Performance tests are run using ``make performancecheck''.</para>

<para>Performance testing is the responsibility of the systems tester.</para>

</sect2>

<sect2><title>Stability Testing</title>

<para>Stability testing validates the reliability, integrity, and robustness of the WDB system. Stability testing involves a number of measures performed while the system is tested in a production-like environment over a designated time period.</para>

<itemizedlist>
    <listitem><para>Resource monitoring</para>
    <para>The resources being used by the WDB system are monitored over a minimum of 7 days of operation with both loading of data and read-access (simulated production) on the system. The system should not show any appreciable growth in memory or CPU usage (i.e., resource leaks) over this period.</para></listitem>
    <listitem><para>Stability</para>
    <para>The WDB system should not crash during simulated production over the designated time period.</para></listitem>
    <listitem><para>Database integrity</para>
      <para>Database metrics (storage space, index extents, etc) should stabilize at expected levels during simulated production over the designated time period.</para></listitem>
</itemizedlist>

<para>Stability tests are started up using ``make stabilitycheck''.</para>

<para>Stability testing is the responsibility of the systems tester.</para>

</sect2>

</sect1>

<sect1><title>Test Process</title>

<para>The purpose of the test process is to verify that the program component fulfills the system requirements specified by the requirements on the GribLoad component.</para>

<sect2><title>Test Process Sequence</title>

<para>The test process consists of the following basic steps.</para>

<orderedlist>
   <listitem><para>Document the requirements</para></listitem>
   <listitem><para>Document the test cases in the system test specification</para></listitem>
   <listitem><para>Peer Review of Requirements and Tests</para></listitem>
   <listitem><para>Implement the prioritized test cases</para></listitem>
   <listitem><para>Pass quality check</para></listitem>
   <listitem><para>Pass distribution check</para></listitem>
   <listitem><para>Pass performance check</para></listitem>
   <listitem><para>Pass stability check</para></listitem>
</orderedlist>

</sect2>

<sect2><title>Documentation of Requirements</title>

<para>The requirements of each component of WDB are documented in Bugzilla. Each requirement should contain the following information:</para>

	<table frame="all">
	  <title>Requirements Attributes</title>
	  <tgroup cols="2">
	    <colspec colname="req1" align="left"/> <colspec
	    colname="req2" align="left"/>
	    <tbody>
	      <row class="oddrow">
		<entry>Requirement Type</entry> <entry>Tag the summary according to the type of requirement in order to facilitate sorting and searching</entry>
	      </row>
	      <row class="evenrow">
		<entry>Summary</entry> <entry>The summary should describe the problem concisely</entry>
	      </row>
	      <row class="oddrow">
		<entry>Description</entry> <entry>A more detailed description of the problem</entry>
	    </row>
	      <row class="evenrow">
		<entry>Fit Criterion</entry>
		<entry>The fit criterion should describe a measurable, unambiguous criteria for stating when the requirement can be considered successfully resolved. Every requirements description should contain a fit criterion.</entry>
	      </row>
<row class="oddrow">
<entry>Dependencies</entry>
<entry>The list of ``lower-level'' requirements or bugs that must be resolved in order for the requirement to be resolved.</entry>
</row>
	    </tbody>
	  </tgroup>
	</table>

</sect2>

<sect2><title>Documentation of Test Cases</title>

<para>The test cases for each component are described and maintained in the system test specification for that component. Ideally, each requirement of a component that is not dependent on lower-level requirements, should have a test case matchings it fit criterion.</para>

<para>Only the installation tests, performance tests, and stability tests are documented using the system test specification. Unit testing is documented in the code (using Javadoc), while the distribution testing follows the GNU distcheck standard.</para>

<para>In addition to the description of the test case and the results expected, the test should also specify the priority of the test case; i.e., rated on the usual three-point scale: Must be implemented, Should be implemented, or Could be implemented.</para>

</sect2>

<sect2><title>Peer Review of Requirements and System Test Specification</title>

<para>The requirements for each component must be reviewed for completeness, conciseness, and correctness by the person(s) responsible for the component.</para>

<para>The test specifications must be submitted to a peer review, in order to ensure completeness of the testing and the accuracy of the prioritization.</para>

</sect2>

<sect2><title>Implement the prioritized test cases</title>

<para>All test cases with priority 1 must be implemented by the developers. Test cases of priority 2 should be implemented, but may be ignored if the situation warrants it. Test cases of priority 3 can be ignored entirely, but are useful to have implemented for the sake of completeness.</para>

<para>All tests must be automated and integrated into the autotools framework utilized by the WDB. Scripts for the tests specified are located in the ``tests'' subdirectory of the subversion server.</para>

</sect2>

<sect2><title>Pass Quality Check</title>

<para>The quality check consists of the compilation tests, unit tests, and the code review.</para>

<para>It is the responsibility of the developer to ensure that compilation tests and unit tests are run before submitting the code to the source repository. Once code has been implemented and committed to the source repository, it must be reviewed by the other members of the project.</para>

<para>The results of the code review are posted to the appropriate mailing list.</para>

</sect2>

<sect2><title>Pass Distribution Check</title>

<para>The distribution check carries out the installation and distribution tests, as implemented using the GNU Autotools distcheck function. This packages, compiles, and installs the entire wdb package, and subsequently runs the unit and installation checks.</para>

<para>A distribution check is performed automatically whenever changes are committed to the WDB source repository. Errors are sent automatically to the relevant project members.</para>

</sect2>

<sect2><title>Pass Performance Check</title>

<para>The performance check consists of the performance tests of the WDB system. These are performed on the WDB tests systems, by the systems tester.</para>

<para>The results of the performance tests are posted to the associated performance benchmark mailing list.</para>

</sect2>

<sect2><title>Pass Stability Check</title>

<para>The stability check consists of the performance of the stability tests of the entire integrated WDB system performed over a specified time period. It should always be performed as the last check before a release.</para>

<para>The results of the stability tests are reported on the associated mailing list.</para>

</sect2>

</sect1>

<sect1><title>Test Administration</title>

<para>The following section details the documentation requirements for testing and the acceptance criteria.</para>

<sect2><title>Test Documentation</title>

<para>The results of tests are documented on the mailing list reports and Bugzilla.</para>

<para>Automatic checks generate an error log, listing the tests carried out and describing any errors that may have occured during the test run.</para>

<para>Prior to a release, the Test Leader should consult the test logs, ensure that all unresolved errors are logged in Bugzilla, and produce a test report that summarizes the test work, lists the fixed problems, and lists any remaining problems, as well as recommendation for the release.</para>

</sect2>

<sect2><title>Acceptance Criteria</title>

<para>Before the acceptance tests can begin, the following criteria should be fulfilled:</para>

<itemizedlist>
    <listitem><para>A compilable version of the system is committed to the version management system of WDB</para></listitem>
    <listitem><para>The code has been quality-checked; either through pair-programming following the WDB coding standards, or by passing a formal code review</para></listitem>
    <listitem><para>System developer tests has performed informal testing on new components/changes</para></listitem>
</itemizedlist>

<para>All errors, deviations from expected results, and other problems of note that are encountered during the test process must be document in the Error log. If these are not fixed immediately, it is the responsibility of the tester to log the bug in Bugzilla, assigning it a severity according to the following criteria.</para>

<itemizedlist>
    <listitem><para>Critical - The Test Process can not be continued.</para></listitem>
    <listitem><para>Major - Error or deviation that must fixed before going into production, but which does not prevent the completion of the test.</para></listitem>
    <listitem><para>Average - Error or deviation that may cause problems in production, but which does not prevent the completion of the test.</para></listitem>
    <listitem><para>Minor - Error or deviation of little or cosmetic importance.</para></listitem>
    <listitem><para>Enhancement - Addition or change to the existing test specification is required.</para></listitem>
</itemizedlist>

<para>All errors and deviation of the first and second type must be corrected, and verified as such through a repeat test. Problems of the last class should result in a change request on the System Test Specification. All changes must be prioritized, and their status tracked. Changes that are not implemented must be stated as such in the concluding test report.</para>

</sect2>
</sect1>

</chapter>